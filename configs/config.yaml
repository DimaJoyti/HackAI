# LLM Security Proxy - Base Configuration Template
# This file serves as a template for environment-specific configurations

# Server Configuration
server:
  port: 8080
  host: "0.0.0.0"
  read_timeout: "30s"
  write_timeout: "30s"
  idle_timeout: "60s"
  shutdown_timeout: "30s"
  tls_enabled: false
  cert_file: ""
  key_file: ""
  cors:
    enabled: true
    allowed_origins: ["*"]
    allowed_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
    allowed_headers: ["Content-Type", "Authorization", "X-Provider", "X-Model"]
    max_age: 3600
  rate_limit:
    enabled: true
    requests_per_minute: 1000
    burst_size: 100

# Database Configuration
database:
  host: "localhost"
  port: "5432"
  name: "hackai"
  user: "postgres"
  password: "password"
  ssl_mode: "disable"
  max_open_conns: 25
  max_idle_conns: 5
  conn_max_lifetime: "5m"
  conn_max_idle_time: "1m"
  migration_path: "./migrations"
  auto_migrate: true

# Redis Configuration (for caching and rate limiting)
redis:
  host: "localhost"
  port: "6379"
  password: ""
  db: 0
  pool_size: 10
  min_idle_conns: 5
  dial_timeout: "5s"
  read_timeout: "3s"
  write_timeout: "3s"
  pool_timeout: "4s"
  idle_timeout: "5m"

# JWT Configuration
jwt:
  secret: "your-secret-key-change-in-production"
  issuer: "hackai-llm-security-proxy"
  audience: "hackai-users"
  access_token_duration: "15m"
  refresh_token_duration: "24h"
  algorithm: "HS256"

# Security Configuration
security:
  enabled: true
  strict_mode: false
  default_threat_score: 0.0
  block_high_threat_score: true
  threat_score_threshold: 0.8
  
  # Policy Engine Configuration
  policy_engine:
    enabled: true
    default_action: "allow"
    evaluation_timeout: "5s"
    cache_enabled: true
    cache_ttl: "5m"
    max_policy_size: "1MB"
    
  # Content Filter Configuration
  content_filter:
    enabled: true
    max_content_length: 100000
    scan_timeout: "10s"
    block_malicious_content: true
    block_pii: true
    block_toxic_content: true
    toxicity_threshold: 0.7
    
  # Rate Limiter Configuration
  rate_limiter:
    enabled: true
    strict_mode: false
    global_requests_per_minute: 10000
    global_requests_per_hour: 100000
    global_tokens_per_minute: 1000000
    global_tokens_per_hour: 10000000
    user_requests_per_minute: 60
    user_requests_per_hour: 1000
    user_requests_per_day: 10000
    user_tokens_per_minute: 10000
    user_tokens_per_hour: 100000
    user_tokens_per_day: 1000000
    user_cost_per_hour: 100.0
    user_cost_per_day: 500.0
    user_cost_per_month: 2000.0
    allow_burst: true
    burst_multiplier: 2.0
    burst_window_seconds: 60
    window_size: "1h"
    window_granularity: "1m"

# Audit Configuration
audit:
  enabled: true
  
  # Audit Logger Configuration
  logger:
    enabled: true
    batch_size: 100
    flush_interval: "30s"
    max_queue_size: 10000
    include_request_body: true
    include_response_body: true
    mask_sensitive_data: true
    compress_large_payloads: true
    max_payload_size: "1MB"
    retention_days: 90
    
  # Audit Middleware Configuration
  middleware:
    enabled: true
    log_successful_requests: true
    log_failed_requests: true
    log_blocked_requests: true
    log_security_violations: true
    log_policy_decisions: true
    excluded_endpoints: ["/health", "/metrics"]
    excluded_providers: []
    min_threat_score_to_log: 0.0
    sample_rate: 1.0
    
  # Audit Service Configuration
  service:
    enabled: true
    retention_period: "90d"
    cleanup_interval: "24h"
    archive_old_logs: true
    archive_threshold: "30d"
    enable_metrics: true
    metrics_update_interval: "5m"
    enable_alerts: true

# Observability Configuration
observability:
  # Logging Configuration
  logging:
    level: "info"
    format: "json"
    output: "stdout"
    file_path: ""
    add_source: true
    time_format: "RFC3339"
    
  # Metrics Configuration
  metrics:
    enabled: true
    port: 9090
    path: "/metrics"
    namespace: "hackai"
    subsystem: "llm_security_proxy"
    
  # Tracing Configuration
  tracing:
    enabled: false
    service_name: "llm-security-proxy"
    endpoint: ""
    sample_rate: 1.0
    insecure: true
    headers: {}

# AI/LLM Provider Configuration
ai:
  providers:
    openai:
      enabled: true
      api_key: ""
      base_url: "https://api.openai.com/v1"
      timeout: "30s"
      max_retries: 3
      models:
        - "gpt-3.5-turbo"
        - "gpt-4"
        - "gpt-4-turbo"
    anthropic:
      enabled: false
      api_key: ""
      base_url: "https://api.anthropic.com"
      timeout: "30s"
      max_retries: 3
      models:
        - "claude-3-sonnet"
        - "claude-3-opus"
    azure:
      enabled: false
      api_key: ""
      endpoint: ""
      api_version: "2023-12-01-preview"
      timeout: "30s"
      max_retries: 3
      
  # Default LLM Settings
  defaults:
    max_tokens: 4096
    temperature: 0.7
    top_p: 1.0
    frequency_penalty: 0.0
    presence_penalty: 0.0
    timeout: "30s"
    
  # Cost Management
  cost_tracking:
    enabled: true
    currency: "USD"
    alert_threshold: 100.0
    daily_limit: 1000.0
    monthly_limit: 10000.0

# Feature Flags
features:
  enable_real_time_dashboard: true
  enable_policy_testing: true
  enable_threat_simulation: false
  enable_advanced_analytics: true
  enable_export_functionality: true
  enable_webhook_notifications: false
  enable_slack_integration: false
  enable_email_alerts: true

# Notification Configuration
notifications:
  email:
    enabled: false
    smtp_host: ""
    smtp_port: 587
    username: ""
    password: ""
    from_address: "noreply@hackai.dev"
    tls_enabled: true
    
  slack:
    enabled: false
    webhook_url: ""
    channel: "#security-alerts"
    username: "HackAI Security Bot"
    
  webhook:
    enabled: false
    url: ""
    timeout: "10s"
    retry_attempts: 3
    headers: {}

# Development/Debug Configuration
debug:
  enabled: false
  pprof_enabled: false
  pprof_port: 6060
  verbose_logging: false
  log_sql_queries: false
  disable_auth: false
  mock_providers: false
